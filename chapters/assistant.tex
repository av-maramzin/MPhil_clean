\chapter{Software Parallelization Assistant}
\label{assistant}
\section{Introduction}

% problem: the need for manual parallelization, auto-parallelisation does not deliver desired performance improvements

\quad Parallel hardware is ubiquitous through the entire spectrum of computing systems, from low-end embedded devices to high-end supercomputers.
%
Yet, most of the existing software is written in a sequential fashion.
%
Despite decades of intensive research in automatic software
parallelization~\cite{6813266}, fully exploiting the potential of modern multi- and many-core hardware still requires a significant manual effort.
%
Given the difficulty of the obstacles faced by automatic parallelization today, we do not expect that programmers will be liberated from performing manual parallelization in the near future~\cite{Larsen:2012:PML:2410141.2410600}.

% solution: parallelisation assistant

This chapter introduces a novel parallelization assistant that aids a programmer in the process of parallelizing a program in the frequent case where automatic approaches fail to do so.
%
The assistant reduces the manual effort in this process by presenting a programmer with a ranking of program loops that are most likely to 1) require little or no effort for successful parallelization and 2) improve the program's performance when parallelized.
%
Thus, it improves over the traditional, profile-guided process by also taking into account the \emph{probability} of potential parallelization for each of the profiled loops.

% how?

At the core of our parallelization assistant resides a novel machine-learning (ML) model of loop parallelizability.
%
Loops are compelling candidates for parallelization, as they are naturally decomposable and tend to capture most of the execution time in a program.
%
Furthermore, focusing on loops allows the model to leverage a large amount of specific analyses available in modern compilers, such as generalized iterator recognition~\cite{Manilov:2018:GPI:3178372.3179511} and loop dependence analysis~\cite{Jensen:2017:ILD:3132652.3095754}.
%
The model encodes the results of these analyses together with basic properties of the loops as machine learning \textit{features}.

The loop parallelizability model is trained, validated, and tested on 1415 loops
from the SNU NAS Parallel Benchmarks (SNU
NPB)~\cite{Seo:2011:PCN:2357490.2358063}.
%
The loops are labelled using a combination of expert OpenMP~\cite{Dagum:1998:OIA:615255.615542}
annotations and optimization reports from Intel \cpp{} Compiler (ICC), a
production-quality parallelizing compiler.
%
The model is evaluated on multiple machine learning algorithms, including tree-based methods, support vector machines, and neural networks.
%
The evaluation shows that -- despite the limited size of the data set -- using
support vector machines allows the model to achieve a prediction accuracy higher than 90\%.
%
% Note: the "detected parallelism increase" is 945/995 - 812/995 = 0.1336683417.
%
The model improves over the ICC Compiler across the sequential C version of the SNU NPB suite by detecting 13\% more parallel loops. Albeit this improvement comes at the cost of introducing \textit{false positives}, where non-parallelizable loops are misclassified as parallelizable. However, the false positive rate in our evaluation is as low as 6.5\%. We feel this is acceptable, as our parallelization assistant does not automatically restructure code but leaves the parallelization decision in the hands of the programmers.
%\todo{(FIXME: taken from~Figure~\ref{fig:accuracy_loocv}, double-check these numbers)}

% TODO: mention that we use standard techniques for feature extraction,
% hyper-parameter tuning, and cross-validation?

The parallelization assistant combines inference on the parallelizability model
with traditional profiling to rank higher those loops with a high probability of being parallelizable and impacting the program performance.
%
An evaluation on eight programs from the SNU NPB suite shows that
the program performance tends to improve faster as loops are parallelized in the ranking order suggested by our parallelization assistant compared to a traditional order based on profiling only.
%
On average, following the order suggested by the assistant reduces by approximately 20\% the number of lines of code a programmer has to examine manually to parallelize SNU NPB to its expert-level speedup.
%
Given the high level of effort involved in manual analysis, such a reduction
can translate into substantial development cost savings.

%% \quad There is a number of well-known problems in the field of parallel software engineering, which compose the ground our work grows on. Parallel hardware is omnipresent in the whole spectrum of computers from low end devices up to the high end supercomputers. Yet, in order to utilise all these available hardware capabilities the software has to be parallel as well. Unfortunately, the legacy software is written in a sequential fashion and in order to make it run faster we either have to parallelise it manually or rely on automatic parallelisation techniques. Both approaches have their pros and cons. The former require parallel programming expertise on top of the application domain knowledge, the latter is limited to the narrow niche of scientific FORTRAN-like codes. Decades of intensive research have not yet delivered a breakthrough, and we do not expect a major breakthrough in automatic parallelisation in the near future. In order to accelerate a software a programmer still has to manually analyse and parallelise it. Indeed, as our experiments show (see section [?]), even the state-of-the-art Intel \cpp{} compiler cannot achieve significant performance improvements on highly parallel by their nature NAS Parallel Benchmarks (NPB). Meanwhile, the field of machine learning (ML) has made a massive progress and continuously finds itself to be effective and practical in a growing set of application areas. ML based techniques are not new to the field of optimising compilers either. As survey [?] shows ML techniques have been applied to tasks ranging from the selection of the best compiler flags to choosing the most optimal mapping of parallelism onto heterogeneous hardware resources.\newline\null
%% \quad In our work we acknowledge the role of a human programmer in the software parallelisation process and develop a supplementary assistant guiding the human efforts by highlighting program loops, which are highly likely to be parallelisable and profitable as well. Our tool targets the phase of parallelism discovery 


%% For several decades, parallelising compilers have been the subject of intensive academic research \cite{XXX} and industrial investment \cite{XXX}. Yet, for most real-world applications they fail to deliver parallel performance \cite{XXX}.


%% In this paper we take a different approach: We acknowledge the role of the human expert in the parallelisation process, and develop a machine learning based assistant guiding the users to focus their resources on those loops most likely to result in a positive return-of-investment (ROI). We do not aim to completely automate parallelisation including program analysis and transformation, but instead assist the user by suggesting an ordered ranking of loops for manual parallelisation, where this ordering considers both the estimated effort and profit of parallelising each loop in turn.


\subsection{Motivating Example}\label{motivating_example}

Consider the sequential C implementation of the \textit{Conjugate Gradient (CG)}
benchmark from the SNU NPB suite.
%
Table~\ref{tab:ranking} shows the top three CG loops as ranked by the Intel
Profiler (based on their execution time) and by our parallelization
assistant (additionally taking into account their parallelizability).
%
Both rankings include the same loops, but, crucially, the loops are \textbf{ranked in a different order}.

\begin{table}
  \begin{minipage}{\columnwidth}
  \begin{center}
    \caption{Comparison of the profiler and assistant rankings for the CG benchmark loops (limited to the top three loops).}
    \begin{tabu}{c|c|cc}
      \hline
      \rowfont{\bfseries}
      \multirow{2}{*}{Ranking} & Profiler & \multicolumn{2}{c}{Assistant} \\ \cline{2-4}
      \rowfont{\bfseries}
      & loop & loop & parallelizability\\\hline
      1 & \texttt{cg.c:326} & \textbf{\texttt{cg.c:509}} & 85\%\\
      2 & \texttt{cg.c:484} & \texttt{cg.c:326} & 29\%\\
      3 & \textbf{\texttt{cg.c:509}} & \texttt{cg.c:484} & 8\%\\\hline
    \end{tabu}
    \caption{Comparison of the profiler and assistant rankings for the CG benchmark loops (limited to the top three loops).}
    \label{tab:ranking}
  \end{center}
  \end{minipage}
\end{table}%

Following the profiler ranking (second column in Table~\ref{tab:ranking}), a
parallelization expert would concentrate on analyzing the loop \texttt{cg.c:326}
first.
%
% Note: the 100 lines of code are counted using 'cloc code/cg-326.c'.
%
Analyzing this loop turns out to be costly (it consists of 100+ lines of code)
and unfruitful (it is actually not parallelizable due to inter-iteration
dependencies and side effects, see Listing~\ref{lst:main_iter}).
%
This analysis would be followed by an equally unfruitful analysis of
loop \texttt{cg.c:484}.

In contrast, our parallelization assistant (two last columns in
Table~\ref{tab:ranking}) ranks the loop \textbf{\texttt{cg.c:509}} before
loops \texttt{cg.c:326} and \texttt{cg.c:484}, as it finds that the former has a
significantly higher probability of being parallelizable (see last column in
Table~\ref{tab:ranking}).
%
Following the assistant's ranking, a parallelization expert would thus
concentrate on analyzing the loop \textbf{\texttt{cg.c:509}} first.
%
Analyzing this loop is inexpensive (it consists of only six lines of code, see
Listing~\ref{lst:reduction}) and fruitful: its parallelization speeds up CG by a
factor of~2.8, which is~70\% of the speedup obtained by parallelizing the entire
benchmark.
%
Hence, following the ranking proposed by our assistant, a parallelization expert
can achieve most of the available speedup in CG in a fraction of the time
required by a traditional profile-guided parallelization process.

\begin{figure}[t]
\begin{lstlisting}[caption={\texttt{cg.c:326}. Longest running loop in CG. The loop cannot be parallelized due to
inter-iteration dependences and side effects caused by system calls.},label={lst:main_iter},language=C]
for (it = 1; it <= NITER; it++) {
  ...
  if (timeron) timer_start(T_conj_grad);
  conj_grad(colidx,rowstr,x,z,a,p,q,r,&rnorm);
  if (timeron) timer_stop(T_conj_grad);
  ...
  printf("    %5d       %20.14E%20.13f\n", it, rnorm, zeta);
  ...
}
\end{lstlisting}

\begin{lstlisting}[caption={\textbf{\texttt{cg.c:509}}. Longest running loop in CG among those \emph{that can be parallelized}.},label={lst:reduction},language=C]
for (j = 0; j < lastrow-firstrow+1; j++) {
  suml = 0.0;
  for (k = rowstr[j]; k < rowstr[j+1]; k++)
    suml = suml + a[k]*p[colidx[k]];
  q[j] = suml;
}
\end{lstlisting}
\end{figure}



\subsection{Contributions}

In summary, this project makes the following contributions:
%
\begin{itemize}
\renewcommand\labelitemi{$\vartriangleright$}
\renewcommand\labelitemii{$\bullet$}
\item We introduce a machine learning model, which can be used to predict the probability with which sequential C loops can be parallelized (Sections~\ref{predicting_parallel_loops} and~\ref{ml_predictive_performance});
\item we integrate profiling of execution time with our novel ML model into a parallelization assistant, which guides the user through a ranked list of loops for parallelization (Section~\ref{practical_applications}); and
\item we demonstrate that our tool and methodology increase programmer productivity by identifying parallel loop candidates better than existing state-of-the-art approaches (Section~\ref{evaluation}).

%Designed an ML based model of loop parallelisability quantifying the latter %(Section~\ref{predicting_parallel_loops}) and ranking program loops
%    \begin{itemize}
%    \item Devised loop parallelisability metrics (ML loop features)
%    \item Prepared a labelled training set with classification labels
%    derived from SNU NPB OpenMP \textit{\#pragmas} as well as Intel Compiler %optimisation reports
%    \item Set and tuned an ML train/test methodology
%    \end{itemize}
%\item Conducted a thorough evaluation of the model demonstrating prediction %accuracy
%  higher than 90\% on~\numprint{\totalLoops{}}~loops from the SNU NAS Parallel
%  Benchmarks (\textcolor{red}{Section~4})
%\item Integrated the model into a parallelisation assistant scheme that %automatically ranks promising loops by combining profiling information and %model inference (\textcolor{red}{Section~5})
%    \begin{itemize}
%    \item Demonstrated improved parallelism discovery capabilities of %assistant in comparison with Intel Compiler 
%    \end{itemize}
%\item Deployment of the assistant on SNU NAS Parallel Benchmarks demonstrating %its potential to ease parallelisation process and converge to the maximum %attainable performance faster
%    \begin{itemize}
%    \item Conducted a performance study of SNU NPB with the state-of-the-art %Intel Compiler
%    \item Parallelisied SNU NPB following profiler and assistant suggested %loop orders and plotted performance convergense curves
%    \end{itemize}
%
% a demonstration of the assistant's potential to reduce the manual effort
%  involved in the traditional profile-guided parallelisation methodology
%  (\textcolor{red}{Section~6}).
\end{itemize}
%% 1) We show that it is possible to learn loop
%% have successfully learnt the parallelisability property o
%% 2) We conduct a study of Intel Compiler's behaviour on SNU NPB benchmarks. We summarise the exact optimizations applied (see section [?]), classify missed parallelisation opportunities (see section [?]) and
%% \subsection{Overview}

%% \subsection{Paper structure}
%% \quad The most substantial part of this work was aimed at the creation of machine learning (ML) based model, capable of accurate prediction of loop parallelisability property. Section \ref{predicting_parallel_loops} describes all the details of machine learning part including the features we chose and the exact training/testing methodology we used. But even close to perfect prediction accuracy is not enough, if we cannot find a real practical application to utilise our predictor. Section \ref{practical_applications} proposes 2 complementary schemes we might integrate our predictor into. The first scheme helps Intel ICC compiler to discover additional parallelism. The second scheme is basically software manual parallelisation assistant working with a program profile and directing human efforts at the most likely to be parallelisible (and profitable as well) loops. Section \ref{evaluation} presents a quantitative report on our work.  

\section{Predicting Parallel Loops}
\label{predicting_parallel_loops}

%\quad This section describes the exact way we applied supervised machine learning (ML) techniques to the problem of loop parallelisability classification. Here we describe the features we choose to represent program loops and the way we collect our data, as well as the whole ML train/test \textit{methodology}. The methodology consists of a number of stages arranged in a \textit{pipeline} (data extraction, data preprocessing, automatic feature selection, ML model selection and the search of its hyper-parameter spaces, training/validation/testing, etc.). All technical background on ML can be found in the book \cite{James:2014:ISL:2517747}.\newline\null
%\quad In order to achieve the best ML performance our features and the exact parameters of methodology have been iteratively tuned and refined with the help of K-fold CV. Predictive \textit{accuracy}, \textit{recall} and \textit{precision} scores were used as selection criteria. To get the most accurate and honest assessment of our ML models we kept the testing data hidden and used only the training data during all ML pipeline stages. Following subsections present the final results rather than the path towards them.


% \subsection{Overview}
% \label{ml_overview}

We approach the prediction of parallel loops as a \emph{supervised probabilistic machine learning classification problem}. Based on sequential reference applications and their manually parallelized counterparts as well as Intel's parallelizing C/C++ compiler, we create a data set of parallelizable and non-parallelizable loops. We extract loop features and use the data set to train a machine learning model, which links feature vectors describing the loops with their observed parallelizability. We then use the trained model as a probabilistic predictor: for each new loop we determine its feature vector and then predict the probability of the loop being parallelizable \cite{Niculescu-Mizil:2005:PGP:1102351.1102430}. For naturally probabilistic models like trees we directly use the computed classification probabilities (fraction of parallelizable training samples in the leaf node). For the support vector machines classifier, we use Platt scaling to derive the probabilities.

In this section we introduce the parallelizability model, whereas Section~\ref{ml_predictive_performance} presents a standard ML performance assessment including accuracy, precision and recall scores. Descriptions and definitions of the machine learning techniques we use can be found in \cite{James:2013:ISL:2517747}. We used the \textit{scikit-learn} library~\cite{scikit-learn} for all ML related tasks.

%\textit{create an ML based model of loop parallelisability and train it to classify loops of Seoul National University implementation \cite{snu-npb-benchmarks} of NAS Parallel Benchmarks \cite{nasa-parallel-benchmarks} as parallelizable or not.}\newline\null 
%\quad We used facilities of \textit{scikit-learn} \cite{scikit-learn} Python library for all ML related tasks. To conduct various ML training/testing experiments in the search of the best ML model we developed a scripting framework taking an input data (train and test) along with an ML pipeline INI configuration file. Configuration file allows us for a flexible change in the settings of an experiment (ML model to use, its hyper-parameter search space, the exact automatic feature selection methods, etc.). The following sections give a detailed description of all ML pipeline stages.

\subsection{Loop Analysis \& Feature Extraction}
\label{loop_analysis_and_features}

%\begin{comment}
%\quad We base our feature engineering efforts on the vast body of work done in the field of optimising compilers. Particularly in the domain of program dependence analysis \cite{Kennedy:2001:OCM:502981}. 
%We dump PDGs in a dot format and systematically use these visualisations in our work in order to debug and decide on the best features to use.
%\end{comment}

For the purpose of machine learning, program loops are represented by numerical \textit{feature vectors}. We derive these features using standard compiler analyses operating on the Program Dependence Graph (PDG)~\cite{Ferrante:1987:PDG:24039.24041} of a loop. The PDG is a representation that captures both data and control information and is constructed using dependence analysis~\cite{Kennedy:2001:OCM:502981}.
\begin{comment}
Figure \ref{fig:pdg} shows an example of a PDG for a simple loop, where SCC stands for \emph{Strongly connected component}.

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{images/pdg_example.pdf}
\caption{Example of PDG of a simple loop with a cross-iteration dependency.}
\label{fig:pdg}
\end{figure}
%\quad One of the most important parts in applying ML techniques to a given problem is the \textit{feature engineering} task. In our project we need to pick the right program loop features, which are the most reflective of loop parallelisability property. In the task of coming up with a set of loop features we are guided by general program dependence analysis theory \cite{Kennedy:2001:OCM:502981}, the exact types of loops present in SNU NPB benchmarks and Intel \cpp{} compiler optimisation reports.\newline\null
%\quad There is a range of SNU NPB loops, which escape Intel compiler parallelisation for different reasons: indirect array references, unrecognised reductions on arrays, pointers with statically unknown memory locations, etc. But all that range of reasons is going to ultimately materialise into data and control dependencies present between loop instructions, represented as edges on the Program Dependence Graph (PDG) \cite{Ferrante:1987:PDG:24039.24041} of a loop. Figure \ref{fig:pdg} shows an example of a PDG built for a simple loop and visualised with our tool.\newline\null

We construct the PDG on a program's LLVM IR and apply standard dependence analysis.
\end{comment}
In addition we use \textit{generalized loop iterator recognition}~\cite{Manilov:2018:GPI:3178372.3179511} to separate \textit{loop iterators} from \textit{loop payloads}. This enables us to define and extract features relating to each of those loop components. In total, we extract a set of 74 static loop features which are based on structural properties of the PDG and the types of instructions constituting it. Table \ref{tab:loop_features} summarizes these features.\newline\null
%\quad The PDG graph consists of LLVM IR instructions as nodes and different sorts of dependencies between them as graph edges. Dependence relation lies at the the very essence of loop parallelisation, on the other hand just counting the number of dependence edges in the PDG of a loop is not enough to make decisions about loop parallelisability. Dependence relations might exist only withing one loop iteration or span across multiple, thus preventing parallelisation.\newline\null
%\quad To refine our features we use the work on \textit{generalised loop iterator recognition} \cite{Manilov:2018:GPI:3178372.3179511}. Generalised iterator recognition analysis separates \textit{loop iterator} from the actual \textit{loop payload} providing us with finer loop partitions to base our features on. As work \cite{Manilov:2018:GPI:3178372.3179511} explains, loop iterator is a \textit{strongly connected component (SCC)} on the PDG with no incoming dependence edges. There are SCCs in the payload as well. Usually they consist of just 1 instruction, but when we have a cross-iteration dependency they tend to grow larger and form a cycle. We call such SCCs \textit{critical}, as ones preventing parallelisation. Figure \ref{fig:pdg} highlights both the iterator and the critical SCC present in the example loop. Inner loop iterators tend to appear as critical SCCs for the outer loop as well. To separate these cases we use \textit{inner loops number} and \textit{loop depth} as separate ML features.\newline\null  
%\quad The feature engineering process has been conducted iteratively and has been guided by the change in ML models predictive performance with the addition of one feature or the other. This process involved several methods. First, we tried to capture in our features the differences between PDG visualisations for parallel and non-parallel loops. Then we studied the source code of SNU NPB benchmarks along with ICC optimization reports and tried to understand why ICC failed to parallelize some of SNU NPB loops and transfer those insights into reflective features.\newline\null
%\quad We ended up with a set of 74 static loop features, which are based on the structural properties of PDG and the types of instructions constituting them. Table \ref{tab:loop_features} summarizes the main groups of devised features.\newline\null
\quad Our features have simple and intuitive motivations behind them. Loop proportion features are backed up by the fact that larger loops tend to be harder to parallelize. Complex iterators include non-trivial cross-iteration transitions (e.g linked-list update), unknown iteration numbers, etc. Payload SCCs (Strongly Connected Components) introduce cross-iteration dependencies. Cohesion features characterize how tightly components of loops are coupled together in terms of the number of edges between them. Loop dependence features count the number of edges in different loop parts as well as their types. Loop instruction features characterize the loop's instruction mix, assigning more importance to memory reads/writes, calls and branches. Non-inlined function calls usually prevent loop parallelization. Intensive memory work (memory read/write fraction features) complicates parallelization as well.

\begin{table*}[!ht]
  \tabulinesep=2pt
  \begin{minipage}{\linewidth}
  \caption{Static features used for the characterization of loops.}
  \begin{center}
    \begin{tabu}{M{3cm}M{5cm}M{8cm}}
      \hline
      \rowfont{\bfseries}
      Feature groups & Features & Description\\\hline
      \multirow{3}{*}{loop proportion} & absolute size & number of LLVM IR instructions\\%\hline
      & payload fraction & $\text{payload instructions} \, / \, \text{total loop instructions}$\\%\hline
      & proper SCCs number & number of payload SCCs with more than 1 instruction\\\hline
      loop dependencies & \multicolumn{2}{M{13cm}}{number of PDG edges for different dependence classes: read/write order (\emph{true}, \emph{anti}, \emph{output}), dependency type (\emph{register}, \emph{memory}, \emph{control}), other (\emph{cross-iteration}, etc.)}\\\hline
      \multirow{2}{*}{loop cohesion} & iterator/payload & $\frac{\text{edges between iterator}/\text{payload}}{\text{total loop edges}}$\\
        & critical/regular payload & $\frac{\text{edges between critical/regular payload}}{\text{total loop payload edges}}$\\
        \hline
      loop instruction nature & \multicolumn{2}{M{13cm}}{numbers and fractions of different parallelization critical instructions (memory loads and stores, branches, calls, etc.)}\\\hline
      \end{tabu}
  \end{center}
  \label{tab:loop_features}
  \end{minipage}
\end{table*}%


%\subsection{Feature Extraction}
%\label{feature_extraction}
%\quad To extract all devised loop features from SNU NPB benchmarks and get the train/test data set we developed \textbf{PParMetrics (Pervasive Parallelism Metrics)} tool, based on the LLVM compiler infrastructure \cite{llvm-compiler-infrastructure}\cite{Lattner:2004:LCF:977395.977673}. The tool is a set of LLVM function passes working on the SSA-based LLVM IR and can be found on the GitHub \cite{github-ppar-tool}. The tool works by building data, memory and control dependence graphs (DDG, MDG, CDG) and combining them into the final program dependence graph (PDG) \cite{Ferrante:1987:PDG:24039.24041} for all loops found in program functions. Once all graphs are built we run the search of strongly-connected components (SCCs) on them and recognise loop iterators. The final step is to traverse all these graphs computing devised metrics (ML features) and dump all that information into the file to be later fed into scikit-learn based ML scripts.


\subsection{Feature Selection}
\label{feature_selection}
%\quad The feature engineering task resulted into a quantitative description of program loops being characterised by feature vectors of length 74. 

To avoid overfitting, we discard irrelevant or redundant features using a pipeline of feature selection methods from the scikit-learn library. First we eliminate features with a low variance score, then we fit a decision tree-based model and select features with importance score above a given threshold. After that we repeatedly run \textit{Recursive Feature Elimination, Cross-Validated (RFECV)} to improve accuracy, precision and recall scores. This yields the final set of features. Table \ref{tab:best_features} presents the 10 highest-ranked features in this set.

%Our ML pipeline scripts can be configured to apply an arbitrary sequence of different scikit-learn feature selection methods. First we filter out all features with a low variance score, then we fit a decision tree based model and select all features with importance score above the threshold. Then we repeatedly run recursive feature elimination by the cross-validation (RFECV) in an attempt to improve several targets: recall, precision and accuracy scores. Table \ref{tab:best_features} illustrates the relative ranking of the 10 highest scoring features in our automatic feature selection runs. SNU NPB benchmarks contain a lot of uninlined function calls and it is unsurprising that the amount of call instructions in the payload of a loop ranks the highest. Despite the absence of straightforward intuition behind cohesion metrics, they tend to correlate with parallelisation labels well. Loops heavy on memory writes also significantly affect the parallelisability property.
\begin{table}[ht]
  \caption{Relative importance of static loop features, ranked by fitting a tree-based ML model.}
  \begin{minipage}{\columnwidth}
  \begin{center}
    \begin{tabu}{lc}
      \hline
      \rowfont{\bfseries}
      \multicolumn{1}{l}{Feature} & \multicolumn{1}{l}{Importance}\\\hline
      payload call fraction & 23.5\\
      iter/payload non-cf cohesion & 18.5\\
      payload mem write fraction & 6.1\\
      loop absolute size & 5.7\\
      critical payload pointer access count & 5.3\\
      payload memory dependence count & 4.0\\
      critical payload non-cf cohesion & 2.9\\
      payload pointer access fraction & 2.7\\
      critical payload total cohesion & 2.6\\\hline
      \end{tabu}
  \end{center}
  \end{minipage}
  \label{tab:best_features}

\end{table}%
\subsection{Model \& Hyper-Parameter Selection}
\label{model_selection}

We evaluate several machine learning classification algorithms in our parallelization assistant, including tree-based methods like decision trees (DT), random forests (RFC) and boosted decision trees (AdaBoost); support vector machines classifiers (SVC) and multi-layer perceptron neural networks (MLP). Section \ref{evaluation_kfold} shows that these models perform similarly with SVC and MLP performing slightly better.

%\quad We use several machine learning algorithms available in the scikit-learn library to compare and find the best one. Among these are tree-based methods like (decision trees (DT), random forests (RFC), boosted decision trees (AdaBoost)), support vector machines (SVC) and neural network based multi-layer perceptron (MLP). Section \ref{evaluation_kfold} shows, that all these models perform comparably good with SVC and MLP being slightly better.
%\subsection{Model Hyper-Parameter Selection}
%\label{model_hyper_parameter_selection}

%\quad For any chosen parametric machine learning model we need to pick the right set of model hyper-parameters. 
For each ML model we use exhaustive hyper-parameter grid search and pick the grid node with the best cross-validation score on the validation set.  The details of all ML pipeline stages are available in our repository \cite{assistant-repo}.
\begin{comment}
We vary grid spaces in an attempt to find the best one. The SVC model is the most sensitive to \textit{C} and $\gamma$ parameters, but it almost always chooses an \textit{rbf} kernel. A multi-layer perceptron works the best with the \textit{relu} activation function, \textit{lbfgs} solver and varying $\alpha$ and network configuration. Tree-based methods tend to keep their growth to relatively small depths (5-15).
\end{comment}

\begin{comment}
\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{2.0cm}|p{5.0cm}|}
        \hline
        model & hyper-parameter space\\
        \hline
        SVC & \textbf{kernel}: rbf,sigmoid,poly, \textbf{C}: 1,10,$10^2$,$10^3$ \textbf{$\gamma$}: 1,$10^{-1}$,$10^{-2}$,$10^{-3}$,$10^{-4}$\\
        \hline
        DT RFC AdaBoost & \textbf{n\_estimators}: 1,2,5,10,50,100 \textbf{max\_depth}: 1,3,5,7,10,15,20,30,50 \textbf{min\_samples\_split}: 0.05,0.1,0.2,0.5,0.7,0.9 \textbf{min\_samples\_leaf}: 1,5,10,15,30,50,60,70,100 \textbf{max\_features}: 1,5,10,15,30,50,70\\
        \hline
        MLP & \textbf{activation}: logistic,relu,tanh \textbf{solver}: lbfgs,sgd,adam \textbf{hidden\_layes\_sizes}: (5;5),(10),(10;10),(10;5),(5;2),(2;5) \textbf{$\alpha$}: $10^2$,10,1,$10^{-1}$,$10^{-2}$\\
        \hline
    \end{tabular}
    \caption{Hyper-parameter spaces to search for an optimal point for different ML models.}
    \label{tab:hyper_param_space}
\end{table}

\begin{table}
  \begin{minipage}{\columnwidth}
  \begin{center}
    \begin{tabu}{ccc}
      \hline
      \rowfont{\bfseries}
      \multicolumn{1}{c}{Model} & \multicolumn{2}{c}{Hyper-Parameter Space}\\\hline
      \multirow{3}{*}{\textbf{SVC}} & \textit{kernel:} & rbf, sigmoid, poly\\%\hline
      & \textit{C:} & 1, 10, $10^2$, $10^3$\\
      & \textit{$\gamma$:} & 1, $10^{-1}$, $10^{-2}$, $10^{-3}$, $10^{-4}$\\\hline
      \multirow{5}{*}{\textbf{\shortstack[c]{DT\\AdaBoost\\RFC}}} & \textit{n\_estimators:} & 1, 2, 5, 10, 50, 100\\%\hline
      & \textit{max\_depth:} & 1, 3, 5, 7, 10, 15, 20, 50\\
      & \textit{min\_samples\_split:} & 0.05, 0.1, 0.2, 0.5, 0.7, 0.9\\
      & \textit{min\_samples\_leaf:} & 1, 5, 10, 15, 30, 50, 70, 100\\ 
      & \textit{max\_features:} & 1, 5, 10, 15, 30, 50, 70\\\hline
    \end{tabu}
  \end{center}
  \end{minipage}
  \caption{Hyper-parameter spaces to search for an optimal point for different ML models.} 
  \label{tab:hyper_param_space}
\end{table}%

\begin{table}
  \caption{Hyper-parameter search spaces for various ML models. Only diapason is shown for ease of visualization.} 
  \begin{minipage}{\columnwidth}
  \begin{center}
    \begin{tabu}{ccc}
      \hline
      \rowfont{\bfseries}
      \multicolumn{1}{c}{Model} & \multicolumn{2}{c}{Hyper-Parameter Space}\\\hline
      \multirow{3}{*}{\textbf{SVC}} & \textit{kernel:} & rbf, sigmoid, poly\\%\hline
      & \textit{C:} & 1 ... $10^3$\\
      & \textit{$\gamma$:} & 1 ... $10^{-4}$\\\hline
      \multirow{5}{*}{\textbf{\shortstack[c]{DT\\AdaBoost\\RFC}}} & \textit{n\_estimators:} & 1 ... 100\\%\hline
      & \textit{max\_depth:} & 1 ... 50\\
      & \textit{min\_samples\_split:} & 0.05 ... 0.9\\
      & \textit{min\_samples\_leaf:} & 1 ... 100\\ 
      & \textit{max\_features:} & 1 ... 70\\\hline
    \end{tabu}
  \end{center}
  \end{minipage}
  \label{tab:hyper_param_space}
\end{table}%
\end{comment}

   

\begin{comment}
Model hyper-parameter tuning process consists of several parts. First, we determine the most important hyper-parameters for each ML model being used. Then, we build a grid with these hyper-parameter ranges. To assess each combination of hyper-parameter values, represented as a point on the grid we perform a standard process. We split our entire set of SNU NAS loops into training and testing subsets. We then take the training subset and further partition it into K different folds. For each fold we train the model with a set hyper-parameters on the remaining K-1 folds and test it on the chosen fold. We average calculated prediction accuracy scores and pick the best one for each hyper-parameter grid point. Then we take the combination of hyper-parameters, which corresponds to the best score.\newline\null
\quad Once the best performing set of hyper-parameters is chosen using CV on the training set, we train ML model with these hyper-parameters on the whole training set and subsequently test it on the testing set chosen at the very beginning.
\end{comment}

\subsection{Training Data \& ML Model Training}
\label{loop_classification_labels}

% \quad In order to train and test our ML model in a supervised way, we need to provide it with the "right answers" regarding loop parallelisability. The task of getting classification labels for loops of SNU NPB benchmarks is complicated by several factors.\newline\null

For training our ML model we use a total of 1415 loops from the SNU NPB benchmark suite. Out of those loops, 210 have been annotated by (external) human experts with parallel OpenMP \textit{pragmas}. We use these annotations as labelled data to indicate parallelizable loops. However, the data is not complete. Human programmers strive to capture only coarse-grain parallelism and do not annotate every parallelizable loop. Hence, we augment the training data with the help of the ICC Compiler, which finds additional parallelizable loops. We combine the results into our training set comprising a total 1415 loops, of which 995 are labelled as parallelizable.  We then use K-fold and Leave-One-Out Cross-Validation (LOOCV) methodologies to train and test our ML models.


%and leave a lot of parallelisable loops (parallelisation of which deems unprofitable) unannotated. By using only these 210 "yes" labels, we are risking to mislead our ML model, since it uses only static program features reflecting dependence-based algorithmic parallelisability of program loops, which do not reflect the profitability of their parallelisation. Moreover, the data set is rather unbalanced (210 "yes" vs 1205 "no"). The latter sets the baseline predictive performance we are going to compete with to a very high level of 85\%. The work \cite{fried_ea:2013:icmla} suffers from this. And seems to incline to classify bigger loops as parallelisable. Given the parallel nature of big loops in SNU NPB, that results into high prediction accuracy, which might not hold for a different set of benchmarks.\newline\null
%\quad Due to above considerations we blend an additional knowledge into our parallelisability labels. We use optimisation reports of the Intel compiler as a second source of information. To extract loop parallelisability labels from the Intel compiler's optimisation reports we developed an optimisation report parser \cite{github-icc-parser}. The task presented us with a number of technical challenges. Before ICC can actually parallelise or vectorise a loop, it applies a number of enabling loop transformations such as loop interchange, distribution, tiling, etc. The detailed description of all these transformations can be found in the paper \cite{Bacon:1994:CTH:197405.197406}. Applied to a loop nest, these optimisations might significantly restructure and distribute the parts of a loop across the whole ICC optimisation report. Moreover, ICC might parallelise only certain parts of transformed loop. At the end we considered a loop to be parallelisible by the ICC compiler if the latter hasn't found any dependencies and either vectorised or parallelised it. In the case of distributed loops, all parts must be parallelisible for an original loop to be considered as such. For a final correctness we conducted a manual verification on top of automatically extracted results.\newline\null
%\quad Table \ref{tab:icc_stats} presents a parsing report, which summarises the number of times ICC applied a certain optimisation. The major cells are \textit{parallel} and \textit{icc}, which report the total number of truly parallelisible loops and the number of loops parallelised by ICC. As it can be seen ICC dompiler does not exploit all the parallelism available in SNU NPB benchmarks. Section \ref{evaluation_icc_competition} presents a study of reasons ICC fails to parallelise certain loops.
%\begin{comment}
%\begin{table}[h!]
%    \centering
%    \begin{tabular}[c]{|p{1.7cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|}
%        \hline
%        total loops & 1415 & parallelised & 653\\
%        \hline
%        parallel & 995 & vectorized & 737\\
%        \hline
%        icc & 812 & parallel deps & 535\\
%        \hline
%        openmp & 210 & vector deps & 266\\
%        \hline
%        distrs & 34 & fusions & 214\\
%        \hline
%        collapses & 58 & tilings & 27\\
%        \hline
%    \end{tabular}
%    \caption{SNU NPB ICC and OpenMP parallelisation statistics. The true %parallel labels are denoted by \textit{parallel}. Number of loops parallelised %by ICC is \textit{icc}. Remaining cells report on different kinds of %optimizations done and reported by the ICC.}
%    \label{tab:icc_stats}
%\end{table}
%\end{comment}
%\begin{table}
%  \begin{minipage}{\columnwidth}
%  \begin{center}
%    \begin{tabu}{cccccc}
%      \hline
%      \rowfont{\bfseries}
%      \multicolumn{2}{c}{\multirow{2}{*}{Labels}} & \multicolumn{4}{c}{Intel %Compiler (ICC)} \\%\hline
%      \rowfont{\bfseries}
%      & & \multicolumn{2}{c}{Optimisation} & %\multicolumn{2}{c}{Parallelisation}\\\hline
%      %loop & ranking & loop & ranking & parallel \\\hline
%      \textbf{total loops} & \textbf{1415} & distrs & 34 & parallel & 653\\
%      \textbf{parallel} & \textbf{995} & fusions & 214 & vector & 737\\
%      \textbf{icc} & \textbf{812} & collapses & 58 & parallel deps & 535\\
%      \textbf{openmp} & \textbf{210} & tilings & 27 & vector deps & %266\\\hline
%    \end{tabu}
%  \end{center}
%  \end{minipage}
%  \caption{Loop classification labels report.}
%  %\caption{Report on loop classification labels derived out of expertly added %OpenMP annotations of SNU NPB benchmarks and ICC optimisation reports. Out of %995 parallelisable loops ICC discovered and parallelised 812.}
%  \label{tab:icc_stats}
%\end{table}%


% \subsection{ML Model Training}
% \label{train_test_methodologies}



%Both have their specific goals and properties.\newline\null
%\quad K-fold CV method blends all the loops from all SNU NPB benchmarks together in the single set and divides it into K equally sized splits. After that the method uses all the possible combinations of K-1 splits to train a model and test it against the one remaining split. Resultant accuracies are averaged to produce the final score. The advantage of that method is that it uses loops from all SNU NPB benchmarks for training and testing. In other words, SNU NPB benchmarks differ in their nature and we use all the available cases and properties to train a model and do not miss any of the information. That leads us to a better overall score. We use that method to tune our model and report its general performance for the same reason as well.\newline\null
%\quad But if we want to utilise our ML models in different practical scenarios (see section \ref{practical_applications}) we have to use separate SNU NPB benchmarks as the whole during model testing. To accomplish that we employ a modified LOOCV method to estimate the predictive performance our models can achieve against separate benchmarks in the set. Here we take all loops in every single benchmark as a testing set and train the model on all loops of 9 remaining benchmarks. The disadvantage of that method is that we exclude the whole benchmark out of the training process. If benchmark has a different nature from the ones we used to train the model, then that comes at the price of reduced accuracy.

\section{ML Predictive Performance}
\label{ml_predictive_performance}

In our work we employ two cross-validation (CV) techniques. We evaluate the overall predictive performance our trained ML model is capable of achieving on SNU NPB benchmarks using K-fold CV. To deploy our assistant against single benchmarks of the suite and assess its effectiveness (Section \ref{evaluation}) we have to use a modified Leave-One-Out CV.

%\quad This section reports on the predictive performance of ML based loop parallelisability model described in the previous section. As has already been described, before we conduct our performance assessment experiments, we have tuned all the parameters of our train/test methodology with the help of K-fold CV and show only the final results.    
%\begin{comment}
%We have 10 SNU NAS benchmarks. We choose one of them and run the oracle training pipeline on the 9 remaining ones. Once the oracle is trained we test it using the chosen unseen benchmark and get parallelisability feedback. Table reports on the performance of out LOOCV method. 
%\end{comment}

\subsection{Overall Model Performance}
\label{evaluation_kfold}
Table \ref{tab:average_accuracy_models} shows the overall predictive performance of different ML models measured with K-fold CV on the whole SNU NPB data set. Training and testing have been done for different values of K (5, 10, 15, 20, 25, 30) and the accuracy remains stable across the entire range. The same is true of recall and precision scores. We used the baselines (constant "parallelizable" prediction and uniform) available in scikit-learn to compare our models against.
\begin{comment}
\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{images/prediction_accuracy_kfold}
\caption{Prediction accuracy measured by K-fold CV on the whole SNU NPB 1415 loop data set as a function of K.}
\label{fig:accuracy_kfold}
\end{figure}
\end{comment}
%\begin{comment}
%\begin{table}[h!]
%    \centering
%    \begin{tabular}[c]{|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
%        \hline
%        ML model & accuracy & recall & precision \\
%        \hline
%        constant & 70.32 & 100 & 70.32\\
%        \hline
%        uniform & 46.27 & 41.50 & 69.79\\
%        \hline
%        SVC & 90.04 & 95.24 & 91.06 \\
%        \hline
%        AdaBoost & 86.96 & 92.92 & 89.06 \\
%        \hline
%        DT & 84.36 & 89.57 & 87.90 \\
%        \hline
%        RFC & 86.65 & 93.22 & 88.47 \\
%        \hline
%        MLP & 89.40 & 93.77 & 91.39 \\
%        \hline
%    \end{tabular}
%    \caption{Average predictive performance for different ML models measured %with a k-fold CV method on the whole set of 1415 SNU NPB loops.}
%    \label{tab:average_accuracy}
%\end{table}
%\end{comment}
\begin{table}
  \caption{Average predictive performance of different ML models measured with K-fold CV on the whole set of 1415 SNU NPB loops.}
  \begin{minipage}{\columnwidth}
  \begin{center}
    \begin{tabu}{cccc}
      \hline
      \rowfont{\bfseries}
      ML model & accuracy & recall & precision\\\hline
      constant & 70.32 & 100 & 70.32\\
      uniform & 46.27 & 41.50 & 69.79\\
      SVC & 90.04 & 95.24 & 91.06 \\
      AdaBoost & 86.96 & 92.92 & 89.06 \\
      DT & 84.36 & 89.57 & 87.90 \\
      RFC & 86.65 & 93.22 & 88.47 \\
      MLP & 89.40 & 93.77 & 91.39 \\\hline
      \end{tabu}
  \end{center}
  \end{minipage}
  \label{tab:average_accuracy_models}
  \vspace{-5mm}
\end{table}%

The SVC model has the highest average accuracy and successfully manages to recall 95.24\% of all parallel loops. The ICC Compiler succeeds in parallelizing 812 out of 995 parallelizable loops available in SNU NPB. Thus, on average SVC extends the ICC Compiler's parallelization capabilities to 948 loops. Figure \ref{fig:prediction_stats} shows that out of the 10\% of mispredictions that SVC makes, 65\% are false positives. Hence, the average unsafe error rate is 6.5\%.
%Thus we need to devise a scheme, that will protect us and make these errors not that critical.
\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{images/prediction_stats.pdf}
\caption{Breakdown of misclassification errors.}
\label{fig:prediction_stats}
\vspace{-5mm}
\end{figure}
\subsection{Model Performance Within Assistant}
\label{evaluation_loocv}
Our proposed assistant (Section \ref{practical_applications}) is trained and tested using LOOCV rather than K-fold CV. Instead of treating the entire set of loops from all SNU NPB benchmarks as a single data set, in this context we train the model on nine benchmarks and test it on the remaining one. Doing so completely excludes the loops of the target benchmark out of a training set, but allows us to get predictions for all benchmark loops, parallelize them if advised so, and test the effectiveness of our assistant. The drawback of this scheme is that it might potentially reduce the accuracy if the nature of loops in the target benchmark dramatically differs from that of loops seen in the training set. Figure \ref{fig:accuracy_loocv_vs_kfold} compares LOOCV accuracy against that of K-Fold CV for all SNU NPB benchmarks, where K-Fold CV is conducted on a data set consisting of loops from a single benchmark only. The comparison proves that lower LOOCV accuracies are attributed to a reduced training data set and not to our ML model.
%\quad While k-fold cross-validation method mixes loops from all 10 SNU NPB benchmarks and provides a good statistical assessment of predictive performance on the whole SNU NPB data set generally, in order to use our predictor with the proposed schemes we need to change our methodology. For that purpose we use modified Leave-One-Out cross-validation (LOOCV) technique. We train our model on 9 benchmarks and test it on the remaining one. Doing so allows us to actually parallelise all correctly predicted parallel loops and get performance numbers for the benchmark being tested.
%\begin{comment}
%\begin{table*}[t!]
%    \centering
%    \begin{tabular}[c]{|p{1.5cm}|p{1.0cm}|p{1.5cm}|p{1.3cm}|p{1.0cm}|p{1.5cm}|%p{1.3cm}|p{1.0cm}|p{1.5cm}|p{1.3cm}|}
%        \hline
%        benchmark & \multicolumn{3}{c}{average per model} \vline &	%\multicolumn{3}{c}{most frequent} \vline & \multicolumn{3}{c}{uniform} \vline %\\
%        \hline
%	    accuracy & recall &	precision &	accuracy & recall &	precision &	%accuracy &	recall	& precision & accuracy \\
%	    \hline
%        BT & 85.7 & 92.6 & 90.0 & 78.4 & 100.0 & 78.4 & 51.9 & 48.3 & 83.3 \\
%        \hline
%        CG & 74.2 & 72.4 & 85.7	& 64.4 & 100.0 & 64.4 & 46.7 & 37.9 & 64.7 %\\
%        \hline
%        DC & 72.8 & 71.0 & 51.5	& 29.0 & 100.0 & 29.0 & 55.0 & 37.9 & 28.9 %\\
%        \hline
%        EP & 92.0 &	86.6 & 100.0 & 60.0 & 100.0 & 60.0 & 40.0 & 33.3 & 50.0 %\\
%        \hline
%        FT & 56.5 &	37.9 & 84.4	& 63.0 & 100.0 & 63.0 & 43.5 & 34.5	& 58.8 %\\
%        \hline
%        IS & 74.0 &	97.5 & 61.3	& 40.0 & 100.0 & 40.0 &	45.0 & 37.5	& 33.3 %\\
%        \hline
%        LU & 90.1 &	90.7 & 96.3	& 78.0 & 100.0 & 78.0 & 46.1 & 45.0	& 76.1 %\\
%        \hline
%        MG & 64.9 &	87.6 & 57.8	& 45.7 & 100.0 & 45.7 &	43.2 & 32.4	& 36.4 %\\
%        \hline
%        SP & 88.8 &	94.7 & 92.2	& 83.4 & 100.0 & 83.4 &	46.6 & 46.4	& 81.7 %\\
%        \hline
%        UA & 75.9 &	83.8 & 83.9	& 72.9 & 100.0 & 72.9 &	48.4 & 47.2	& 72.5 %\\
%        \hline
%    \end{tabular}
%    \caption{Predictive performance on different for different ML models %measured with a k-fold CV method on the whole set of 1415 SNU NPB loops.}
%    \label{tab:average_accuracy}
%\end{table*}
%\end{comment}
\begin{comment}
\begin{table*}{\textwidth}
  \caption{Average predictive performance across different ML models measured with a modified LOOCV method for all SNU NPB benchmarks.} 
  \begin{minipage}{\textwidth}
  \begin{center}
    \begin{tabu}{c|ccc|ccc|ccc}
      \hline
      \rowfont{\bfseries}
      Benchmark & \multicolumn{3}{c}{Average Per Model} \vline & \multicolumn{3}{c}{Most Frequent} \vline & \multicolumn{3}{c}{Uniform}\\\hline
      \rowfont{\bfseries}
      Accuracy & Recall & Precision & Accuracy & Recall & Precision & Accuracy & Recall	& Precision & Accuracy\\\hline
      BT & 85.7 & 92.6 & 90.0 & 78.4 & 100.0 & 78.4 & 51.9 & 48.3 & 83.3 \\
      CG & 74.2 & 72.4 & 85.7	& 64.4 & 100.0 & 64.4 & 46.7 & 37.9 & 64.7 \\
      DC & 72.8 & 71.0 & 51.5	& 29.0 & 100.0 & 29.0 & 55.0 & 37.9 & 28.9 \\
      EP & 92.0 &	86.6 & 100.0 & 60.0 & 100.0 & 60.0 & 40.0 & 33.3 & 50.0 \\
      FT & 56.5 &	37.9 & 84.4	& 63.0 & 100.0 & 63.0 & 43.5 & 34.5	& 58.8 \\
      IS & 74.0 &	97.5 & 61.3	& 40.0 & 100.0 & 40.0 &	45.0 & 37.5	& 33.3 \\
      LU & 90.1 &	90.7 & 96.3	& 78.0 & 100.0 & 78.0 & 46.1 & 45.0	& 76.1 \\
      MG & 64.9 &	87.6 & 57.8	& 45.7 & 100.0 & 45.7 &	43.2 & 32.4	& 36.4 \\
      SP & 88.8 &	94.7 & 92.2	& 83.4 & 100.0 & 83.4 &	46.6 & 46.4	& 81.7 \\
      UA & 75.9 &	83.8 & 83.9	& 72.9 & 100.0 & 72.9 &	48.4 & 47.2	& 72.5 \\\hline
      \end{tabu}
  \end{center}
  \end{minipage}
  \label{tab:average_accuracy_loocv}
\end{table*}%
\end{comment}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{images/LOOCV_accuracy.pdf}
\caption{Prediction accuracy measured with modified LOOCV and compared against that of K-fold CV for single benchmarks.}
\label{fig:accuracy_loocv_vs_kfold}
\vspace{-5mm}
\end{figure}

\section{Parallelization Assistant}
\label{practical_applications}

The ML-based predictor developed and assessed in the previous two sections is a core component of our novel parallelization assistant. This assistant incorporates prediction results on whether a loop can be parallelized and combines this with profiling information. It then produces a ranking of all loops in an application to guide a programmer towards the most beneficial loop candidates for their \textit{manual} parallelization effort. We do not seek to replace the programmers from the process, but aim to assist and increase their productivity.

%\quad Achieving high predictive performance is not enough. We need to find a real practical application of our loop parallelisability predictor. Due to statistical nature inherent to all machine learning techniques it is impossible to eliminate all prediction errors completely. While false negative mispredictions might just miss available parallelisation opportunities and lose some performance, false positive mispredictions can break the program and are the most critical in the context of our ML problem.
%\paragraph{\textbf{Loop Parallelisation Assistant}} Taking the above mentioned fact into consideration we propose to integrate our trained predictor into an assistant scheme, which leaves the final parallelisation decision up to a programmer. Taking application profile as an input our assistant computes a ranking score for every loop in the application. 

\paragraph{Loop Ranking.}
The loop ranking computed by our parallelization assistant combines a loop's contribution to the overall program execution time with its predicted probability of being parallelizable. In particular, we obtain the ranking by applying a shifted sigmoid function to the predicted parallelizability probability multiplied by the application runtime fraction a loop takes to run as shown in Figure \ref{fig:sigmoid_3d}.
\begin{figure}[ht]
\includegraphics[width=0.5\textwidth]{images/product_func.pdf}
\caption{For each loop the ranking function combines its contribution to the application's execution time and its predicted probability of being parallelizable.}
\label{fig:sigmoid_3d}
\end{figure}

The intuition for using this function to combine the two metrics is that it prioritizes parallelizable long-running loops and scales down the weight of non-parallelizable loops irrespective of their contribution to execution time. The effect of this can be seen in Figure \ref{fig:ft_loop_ranking} for the loops of the FT benchmark.

%With a more detailed examination it is visible, that the shape of the function is chosen in a way, that it ranks parallelisable long-running program loops the highest and tries to amplify all non-parallelisable loops down irrespective of their running time. The effect is better demonstrated with the case study presented on the figure \ref{fig:ft_loop_ranking}, which highlights the contrast between different rankings of FT benchmark loops.

\begin{figure}[ht]
\includegraphics[width=0.47\textwidth]{images/ft_filter.pdf}
\caption{Change of loop rankings with the application of the assistant ranking function for the 46 loops of the FT benchmark. Ranking based on loop execution time alone (top figure) results in some high-ranked, but non-parallelizable loops. Combining profiled execution time and parallelizability in a single score (middle figure) results in a ranking that prioritizes parallelizable loops (bottom figure).}
\label{fig:ft_loop_ranking}
\end{figure}

If programmers attempt to parallelize loops in the order prescribed by their execution time, they will inevitably waste
their efforts trying to parallelize loops which may be long-running, but offer little or no opportunity for extracting parallelism. Instead, by taking into account predicted parallelizability our ranking directly guides the programmer towards loops that significantly contribute to overall execution time \textbf{and} offer a realistic prospect of parallelization.

%Top left and right pictures are identical and rank FT benchmark loops according to their running time score (given by the Intel Compiler). The problem with that order is that it ranks long-running program loops the highest independent of their parallelisability. If a programmer starts to parallelise application following that order, he is going to waste his time and efforts on the loops one cannot parallelise anyway. The left and right plots at the bottom of the figure demonstrate a transformation in the loop ranking our assistant does. Vertical axes of these plots correspond to the values our shifted sigmoid ranking function (see figure \ref{fig:sigmoid_3d}) takes. The bottom left plot shows how the relative height of the bars changes with a switch from a loop runtime to our scoring function. It is clear from the plot, that non-parallelisable loops go down in their importance. The bottom right picture shows that such a change actually moves true parallel loops toward the beginning of the list. The longer loop takes to run, the bigger the move. On the other hand non-parallelisable loops move towards the back of the list (almost independent of their running time). The improved ranking of FT loops allows a programmer to immediately start benchmark parallelisation with the most important loops and don't waste time looking at long-running loops, which cannot be parallelised by their nature anyway.
\begin{comment}
\begin{figure}[ht]
\includegraphics[width=0.45\textwidth]{images/icc_competition_scheme.pdf}
\caption{First predictor application. Smart parallelization assistant. }
\label{fig:icc_competition_scheme}
\end{figure}
\end{comment}

\subsection{Comparison to Static Analysis} 

We have compared the generated loop parallelizability classifications of our assistant against that of the ICC Compiler, which due to its use of static analysis is conservative and occasionally misses some parallelization opportunities. The ML approach to parallelization with a human programmer responsible for final code transformation allows our parallelization assistant to be more aggressive than the ICC Compiler. In other words, our model can predict more loops as parallelizable. 
\begin{comment}
The visible effect of this is that our assistant potentially assigns high probabilities to some
possibly-parallel loops and awards them a relatively high rank, despite the fact that the ICC Compiler is not capable of parallelizing them.
\end{comment}

\begin{comment}
\begin{table}[ht]
    \centering
    \caption{All the possible ICC Compiler competition scheme combinations}
    \begin{tabular}[c]{|p{1.0cm}|p{1.5cm}|p{1.5cm}|p{2cm}|}
        \hline
        ICC Compiler & predictor & true parallel & classification bucket \\
        \hline
        0 & 0 & 0 & correct 0 (no) prediction \\
        \hline
        0 & 0 & 1 & missed opportunity \\
        \hline
        0 & 1 & 0 & false positive \\
        \hline
        0 & 1 & 1 & discovery \\        
        \hline
        1 & 0 & 0 & impossible \\
        \hline
        1 & 0 & 1 & icc shielding \\
        \hline
        1 & 1 & 0 & impossible \\
        \hline
        1 & 1 & 1 & correct 1 (yes) prediction \\  
        \hline
    \end{tabular}
    \label{tab:combinations_table}
\end{table}
\end{comment}
\begin{table}
  \tabulinesep=2pt
  \caption{Possible loop classification combinations for the side-by-side setup of our ML predictor and the ICC Compiler.}
  \begin{minipage}{\columnwidth}
  \begin{center}
    \begin{tabu}{M{0.7cm}M{1.4cm}M{1.4cm}M{3.4cm}}
      \hline
      \rowfont{\bfseries}
      ICC & Predictor & True Parallel & Classification\\%\hline
      \hline
      0 & 0 & 0 & icc/predictor agreement\\
      0 & 0 & 1 & missed opportunity\\
      0 & 1 & 0 & false positive\\
      0 & 1 & 1 & discovery\\
      1 & 0 & 0 & impossible\\
      1 & 0 & 1 & icc shielding\\
      1 & 1 & 0 & impossible\\
      1 & 1 & 1 & icc/predictor agreement\\\hline  
    \end{tabu}
  \end{center}
  \end{minipage}
  \label{tab:combinations_table}
\end{table}%

We have set up an experiment where we apply our ML predictor side-by-side with the ICC Compiler. Both aim at independently
classifying loops as parallelizable or not. There is a total of six possible classification combinations that our scheme can produce (summarized in Table \ref{tab:combinations_table}). The first and last table rows show the agreement cases, where the ICC Compiler and the ML predictor identically classify truly (non-)parallelizable loops as (non-)parallelizable. The ``missed opportunity'' cases where both ICC Compiler and ML predictor miss parallelizable loops also represent agreement and are not interesting. The most interesting cases are those where the ML predictor and the ICC Compiler disagree. While ICC Compiler is conservative and will never classify a non-parallelizible loop as parallelizible, the statistical ML predictor can make a ``false positive'' error. That works in the opposite direction as well. The ML predictor can discover truly parallelizible loops which escape compiler analysis. These cases are classified as ``discovery'' and have been manually checked in the source code of SNU NPB. The results are summarized in Table \ref{tab:icc_missed_opportunities}, which reports on the reasons behind ICC conservativeness. False negative mispredictions make the ML predictor miss some real parallelization opportunities, but in the fraction of these cases the ICC Compiler can catch them and ``shield'' the ML predictor.
\begin{table}
  \tabulinesep=2pt
  \caption{Classification of parallelizable loops rejected for parallelization by the ICC Compiler.}
  \begin{minipage}{\columnwidth}
  \begin{center}
    \begin{tabu}{M{1.4cm}M{0.5cm}M{1.6cm}M{0.5cm}M{1.6cm}M{0.5cm}}
      \hline
      \rowfont{\bfseries}
      Reason & Num & Reason & Num & Reason & Num\\\hline
      missed reduction & 18 & array privatization & 7 & conservative analysis & 60\\\hline
      unknown iteration number & 7 & static dependencies & 46 & too complex & 22\\\hline
      non-inlined calls & 4 & other & 4 & total & 168\\\hline
    \end{tabu}
  \end{center}
  \end{minipage}
  \label{tab:icc_missed_opportunities}
  \vspace*{-5mm}
\end{table}%

Figure \ref{fig:icc_competition} shows the relative frequency of different loop classification combinations from Table \ref{tab:combinations_table}. We repeatedly ran K-fold CV on the whole set of SNU NPB loops and sorted the outcomes into separate classification buckets from Table \ref{tab:combinations_table}. In 80\% of the cases, our parallelization assistant agrees with the ICC Compiler and classifies truly (non-)parallel loops as (non-)parallel. This is an expected result as we have used ICC (along with OpenMP annotated loops) to train our ML model. However, sometimes our parallelization assistant and ICC reach different conclusions. While there is agreement in the majority of the cases, our tool discovers an additional 10\% of genuine parallelism inaccessible to the ICC Compiler, while allowing 8\% of false positives.

\begin{figure}[t!]
    %\begin{minipage}{\columnwidth}
    \centering
    %\includegraphics[trim=1mm 1mm 1mm 1mm,clip,width=0.9\columnwidth]{figures/icc_vs_predictor.pdf}
    \includegraphics[width=0.9\columnwidth]{images/pie.pdf}
    \caption{
      Distribution of loop classification by the ICC Compiler and our predictor.}
    \label{fig:icc_competition}
%\end{minipage}
\vspace*{-5mm}
\end{figure}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.95\textwidth]{images/perf_conv_curves_new.pdf}
\caption{From left to right more loops are parallelized for each benchmark. As we parallelize more loops, program execution times improve over the initial sequential performance and reach the performance level of the reference OpenMP implementations. Our ML based parallelization assistant requires the user to parallelize fewer loops than a purely profile-guided approach to reach the maximum parallel speedup.}
\label{fig:performance_convergence_line}
\vspace*{5mm} % Just to fill up the rest of the page
\end{figure*}

\section{Assistant Evaluation}
\label{evaluation}

In this section we evaluate the effectiveness of our parallelization assistant. In particular, we are interested in the potential programmer productivity gains delivered by our tool and savings on human expert time.

Our study assumes that the human expert starts with a sequential version of the SNU NPB benchmarks. The goal
is to parallelize these applications to a performance level matching that of their existing parallel versions.
By using our assistant we expect the human expert to consider fewer loops than by using a profiling-based approach, i.e.\ considering loops in decreasing execution time. We also compare against the state-of-the-art fully automated parallelization approach implemented in the ICC Compiler. The ICC Compiler not only fails to achieve noticeable speedups, but actually slows the performance down on most of the benchmarks. For the BT benchmark the slowdown reaches 3.5 times. In contrast, the OpenMP reference implementation results in a \mbox{(geo-)mean} speedup of 2.19 across the benchmark suite.

Figure~\ref{fig:performance_convergence_line} summarizes our results as
performance convergence curves. For each benchmark the curves plot its execution
time (y-axis) as a function of the number of analyzed and possible parallelized
loops (x-axis). The runtime is bounded within the runtime of the serial
execution (top dashed line) and the time of the reference parallel version (bottom
dashed line). Our goal is to reach the performance of the reference parallel
versions of each program by parallelizing them one loop at a time following the
rankings offered by our assistant and the profiler. The neural network based MLP
model of our assistant provides the fastest overall performance
convergence. While there is some variation depending on the ML model used for
the parallelization prediction, in general ML-assisted parallelization
outperforms or equals the profile-guided schemes across all benchmarks.

Following the rankings of our assistant in parallelizing the BT, CG and FT
benchmarks, we reach their maximum potential performance faster. For BT, maximum
parallel performance can be reached after the user has parallelized the first three 
loops (3061 LOC in Table \ref{tab:parallelization}) suggested by our assistant, while profile-guided parallelization requires 6 loops (6122 LOC) to be parallelized first before reaching the same performance
level. For CG, if we follow the suggestion of our assistant to first parallelize
a small loop (6 LOC), we are able to achieve 70\% of the
maximum potential speedup (see Section~\ref{motivating_example}). On the other
hand, using the profiler ranking requires examining three loops, totalling in
330 LOC, to yield the same performance gains. Moreover, for the SP and UA
benchmarks, some of the assistant rankings require the programmer to examine
more loops than the profiler. However, the loops proposed by the assistant in
the UA benchmark are actually simpler, since they consist of fewer LOC -- 508
LOC for MLP and 579 for DT versus the 882 LOC offered by the profiler. By following our assistant's suggestions, a
programmer would be required to examine 20\% less LOC on average across all models.

%% In general, as it can be seen from Figure \ref{fig:performance_convergence_line} our assistant models perform either better or equal to the profiler. Benchmarks BT, CG and FT gets parallelized to their best OpenMP performance faster by following assistant rankings. In the cases of SP and UA benchmarks some of our assistants require more loops to be examined. However, as Table \ref{tab:parallelization} shows the loops of UA benchmark proposed by our assistants are actually simplier, since they consist of fewer Lines Of Code (LOC) (508 and 579 of MLP and DT against 882 of the profiler). If we calculate the average (across all models) LOC number our assistant would require a programmer to examine and compare that total number against that of a profiler, we see a 20\% reduction.

%% Let's recall a CG benchmark from a motivating example (Section 1). Table \ref{tab:parallelization} shows that the running time of its serial version is 69.38 seconds. SNU NPB provided parallel version takes 19.77 seconds to run. If we follow the ranking of MLP model and parallelize just a small (6 LOC) single loop from the Listing 2, we are going to shorten its execution time to 25.06 seconds (70\% of the maximum speedup). On the contrary, a profiler's ranking would require us to check 3 loops (the total of 330 LOC) to reach the same critical performance level. Full CG benchmark parallelization would bring us 19.77 seconds running time, but we can stop earlier.

In some cases, partial benchmark parallelization might result in a slowdown. In the case of LU, after having parallelized the first 25 loops we do not converge to the best achievable parallel version performance. There are a total of 40 OpenMP pragmas in the benchmark and we need to parallelize all the respective loops to reach the best performance level. In the case of UA, all rankings suggest analyzing a long-running innermost loop first. Its parallelization actually increases running time due to a synchronization barrier being introduced at a wrong program point. It takes 30 loops for the MLP model to achieve the parallel version performance.

\begin{table*}[t]
  \begin{minipage}{\textwidth}
  \begin{center}
\caption{SNU NPB benchmark parallelization reports. The left part of the table shows execution times of serial, OpenMP and partially parallelized (critical) versions. The partially parallelized versions have only several critical (top ranked) loops parallelized. The right hand part of the table shows the number of top-ranked loops one needs to parallelize in order to reach the critical performance. The Profile column gives the reference number a profiler requires. The total lines of code (LOC) in the loops are written down as underscript. In most cases, ML based models converge to the critical performance faster than a profiler based approach (highlighted with green). Red cells show the cases where a profiler outperforms our assistants.}\label{tab:parallelization}
    \begin{tabu}{c|ccc|cc|c|ccccc}
      \hline
      \rowfont{\bfseries}
      \multirow{2}{*}{Bench.} & \multicolumn{3}{c}{Bench.~Runtime, \textit{sec}} \vline & \multicolumn{2}{c}{Speedup, \textit{times}} \vline & \multicolumn{6}{c}{$Loops\ Number_{LOC}$}\\\cline{2-12}
      \rowfont{\bfseries}
      & Serial & OpenMP & Critical & OpenMP & Critical & Profile & SVC & MLP & RFC & AdaBoost & DT\\\hline
      BT & 158.76 & 57.36 & 56.57 & 2.77 & 2.81 & $6_\textit{6122}$ & \cellcolor[HTML]{FA8D8D} $8_\textit{6392}$ & \cellcolor[HTML]{7BB66B} $4_\textit{4088}$ & \cellcolor[HTML]{7BB66B} $5_\textit{5105}$ & \cellcolor[HTML]{7BB66B} $3_\textit{3061}$ & \cellcolor[HTML]{7BB66B} $5_\textit{5105}$\\
      CG & 69.38 & 19.77 & 25.06 & 3.51 & 2.77 & $3_\textit{330}$ & \cellcolor[HTML]{7BB66B} $2_\textit{118}$ & \cellcolor[HTML]{7BB66B} $1_\textit{6}$ & \cellcolor[HTML]{7BB66B} $1_\textit{6}$ & \cellcolor[HTML]{7BB66B} $1_\textit{6}$ & \cellcolor[HTML]{7BB66B} $1_\textit{6}$\\
      DC & 698.82 & 254.29 & 698.82 & 2.75 & 1.00 & $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$\\
      EP & 86.35 & 35.40 & 35.07 & 2.44 & 2.46 & $1_\textit{45}$ & \cellcolor[HTML]{91A1FA} $1_\textit{45}$ & \cellcolor[HTML]{91A1FA}$1_\textit{45}$ & \cellcolor[HTML]{91A1FA} $1_\textit{45}$ & \cellcolor[HTML]{91A1FA} $1_\textit{45}$ & \cellcolor[HTML]{91A1FA} $1_\textit{45}$\\
      FT & 36.81 & 12.13 & 14.69 & 3.03 & 2.51 & $9_\textit{338}$ & \cellcolor[HTML]{7BB66B} $4_\textit{187}$ & \cellcolor[HTML]{7BB66B} $3_\textit{140}$ & \cellcolor[HTML]{7BB66B} $4_\textit{187}$ & \cellcolor[HTML]{91A1FA} $9_\textit{338}$ & \cellcolor[HTML]{7BB66B} $5_\textit{193}$\\
      IS & 4.75 & 1.35 & 4.63 & 3.53 & 1.03 & $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$\\
      LU & 115.46 & 55.00 & 140.53 & 2.10 & 0.82 & $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$ & \cellcolor[HTML]{91A1FA} $\infty$\\
      MG & 5.20 & 3.58 & 3.94 & 1.45 & 1.32 & $3_\textit{43}$ & \cellcolor[HTML]{91A1FA} $3_\textit{43}$ & \cellcolor[HTML]{91A1FA} $3_\textit{43}$ & \cellcolor[HTML]{91A1FA} $3_\textit{43}$ & \cellcolor[HTML]{91A1FA} $3_\textit{43}$ & \cellcolor[HTML]{91A1FA} $3_\textit{43}$\\
      SP & 86.65 & 65.19 & 62.90 & 1.33 & 1.38 & $3_\textit{801}$ & \cellcolor[HTML]{91A1FA} $3_\textit{801}$ & \cellcolor[HTML]{FA8D8D} $\infty$ & \cellcolor[HTML]{91A1FA} $3_\textit{801}$ & \cellcolor[HTML]{91A1FA} $3_\textit{801}$ & \cellcolor[HTML]{FA8D8D} $20_\textit{1257}$\\
      UA & 71.82 & 78.56 & 189.66 & 0.91 & 0.38 & $19_\textit{882}$ & \cellcolor[HTML]{FA8D8D} $30_\textit{918}$ & \cellcolor[HTML]{7BB66B} $30_\textit{508}$ & \cellcolor[HTML]{7BB66B} $19_\textit{861}$ & \cellcolor[HTML]{91A1FA} $22_\textit{883}$ & \cellcolor[HTML]{7BB66B} $10_\textit{579}$\\\hline
      \end{tabu}
  \end{center}
\end{minipage}
  %\vspace*{-5mm}
\end{table*}

\begin{comment}
While there is some variation depending on the ML model used for the parallelization prediction, in general ML assisted parallelization outperforms or equals the profile-guided schemes in all benchmarks.
\end{comment}

%% The DC and IS benchmarks are not shown in Figure \ref{fig:performance_convergence_line}. Neither our parallelization assistant nor the profiler reach the performance of the reference OpenMP versions on these benchmarks. Manual inspection reveals that these benchmarks have been parallized using OpenMP parallel sections, but do not contain any OpenMP parallel loops. Both our parallelization assistant and the profiler incorrectly suggest to parallelize some of the benchmark loops, though. The latter leads to the divergence of performance curves and marked with an infinity $\infty$ sign in Table \ref{tab:parallelization}.

Finally, we observe that neither our parallelization assistant nor the profiler
reach the performance of the reference OpenMP versions on the DC and IS
benchmarks. Manual inspection reveals that these benchmarks have been parallized
using OpenMP parallel sections, but do not contain any OpenMP parallel
loops. Both our parallelization assistant and the profiler incorrectly suggest
to parallelize some of the benchmark loops,
though. Table~\ref{tab:parallelization} summarizes the results of applying our
parallelization assistant to the SNU NPB suite.

%are actually demonstrating the deployment of our assistant on SNU NAS Parallel Benchmarks. But before we can assess our assistant we need to conduct a study of performance one could potentially extract from SNU NPB benchmarks. Having measured running times of differently compiled versions, we discovered that ICC compiler not only fails to achieve noticeable speedups, but actually slows the performance down on most of the benchmarks. For BT benchmark the slowdown is striking 3,5 times. Manual expertly done OpenMP parallelisation of SNU NPB developers results into the geomean speedup of 2.19x.\newline\null
%\quad To conduct SNU NPB assistant deployment experiment we had to use a modified LOOCV. For every single SNU NPB benchmark we used the 9 remaining ones to train the predictor and get the ranking of benchmark loops. As section \label{loocv_accuracy} shows such a methodology has a lower predictive performance due to training set incompleteness, which potentially limits the success of our technique. After that we followed the ranking and parallelised the chosen benchmark loop by loop until we managed to achieve the performance level of an expertly parallelised version. We repeated the process for different ML models as well as for a profile based ranking and plotted the performance convergence curves on figure \ref{fig:performance_convergence_line}. Our technique is not applicable to DC and IS benchmarks: these benchmarks get their parallel speedups from OpenMP parallel sections and not from parallel loops. Parallelisation of loops in these benchmarks (be it profile ordered list or the one of our adviser) actually slows them down.  
%\begin{comment}
%\begin{figure*}[th]
%\centering
%\includegraphics[width=\textwidth]{figures/benchmark_runtime.pdf}
%\caption{SNU NPB running times for different compilation options: serial, %automatic parallelisation and vectorization, OpenMP benchmark expert developer %version and all the possible combinations of those.}
%\label{fig:snu_npb_performance}
%\end{figure*}
%\end{comment}
%\begin{comment}
%\quad The common general thing about all SNU NAS benchmarks we observed is %that the main portion of benchmark speedup comes from a coarse-grain manual %parallelisation done by its developers. The main loops preceded by OpenMP %pragmas are quite big and complex. Usually they contain calls to different %%functions, which do not always get inlined, as well as a lot of %multidimensional arrays with complex index computations. Many array references %happen indirectly and thus require runtime behaviour knowledge for their %analysis. Parallelisation of such loops requires a thorough comprehension of %the source code by a programmer, and is beyond the capabilities of the %state-of-the-art automatic tools.\newline
%\quad Intel compiler vectorises and parallelises quite a significant number of %SNU NAS benchmark loops, but these loops are not the main ones. When we %measure the performance of parallel and vector codes generated by Intel %compiler, we observe running times being slightly better than in serial %versions for vector codes and significant slowdowns for automatic ICC %parallelisation.       
%\quad The major weakness of Intel compiler, as well as our trained loop %parallelisability classifier is that they mostly discover relatively %fine-grain parallelism and miss opportunities seized by manual coarse-grain %parallelisation. SNU NAS benchmarks contain a lot of big loops with deep %nesting and uninlined function calls inside. SNU NAS developers have deep %benchmark behaviour understanding and know exactly where even such loops can %be parallelised, but that knowledge is far beyond the sight of automatic %tools. 
%\quad Parallelism present in EP and DC benchmarks is undetectable neither by %Intel compiler nor by our oracle. DC benchmark contains only 1 parallel %section encapsulating many function calls and complex control flow. Such %parallelism cannot be detected in principle. Performance of EP benchmark %depends heavily on the single loop with a reduction. SNU NPB developers have %successfully manually parallelised this loop, but neither ICC nor trained %oracle could find parallelism in it. Oracle predicted thsi loop to be 40\% %parallelisible. The loop is complex and contains 2 inner loops, where %reduction variables are buried as well as timer and random generator function %calls.\newline
%\quad Our scheme does not utilise all the coarse-grain parallelism of FT %benchmark as well. Developers parallelise outermost loops of loop nests, %whereas our classifier finds parallelism only in a modestly-sized inner loops, %which do not contain function calls (thus operating at a finer level). Such %finer parallelisation is not always beneficial and introduces overheads from %implicit OpenMP barriers.
%\quad Our scheme advised us to parallelise almost all BT benchmark loops, %which contain OpenMP pragma in the original hand-parallelised benchmark %version. There are a couple of missed OpenMP pragmas. Moreover, our scheme %advises us to parallelise inner loops with a finer grains of parallelism, but %doing so results in a slowdown, rather than speedup. So we take predictor's %feedback and apply it on the top of our common sense that only outer loops %should be parallelised to avoid synchronisation overheads and stalls.    
%\end{comment}

\section{Related Work}
\label{related_work}

%We focus our discussion of related work on contributions, which are related to the ML aspects of the approach presented in this paper.

\textit{Profitability Analysis.}
The SUIF \cite{Wilson:1994:SIR:193209.193217} parallelizing compiler uses a simple heuristic based on the product of statements per loop iteration and number of loop iterations to decide whether a parallelizable loop should be scheduled to be executed in parallel. In contrast, Tournavitis \emph{et al.}~\cite{Tournavitis:2009:THA:1542476.1542496} use a machine learning based heuristic, which incorporates \textit{dynamic} program features collected in a separate profiling stage, to decide if and how a potentially parallel loop should be scheduled across multiple processors.

\textit{ML in Compiler Optimization.}
Machine learning has been used to solve a wide range of problems, from the early successful work of selecting compiler flags for sequential programs, to recent works on scheduling and optimizing parallel programs on heterogeneous multi-cores. Some works for machine learning in compilers look at how, or if, a compiler optimization should be applied to a sequential program. Some of the previous studies build supervised classifiers to predict the optimal loop unroll factor \cite{4907653,1402082} or to determine whether a function should be inlined \cite{Zhao2003ToIO,1559966}. These works target a fixed set of compiler options, by representing the optimization problem as a multi-class classification problem where each compiler option is a class. Evolutionary algorithms like generic search are often used to explore a large design space. Prior works~\cite{Almagor:2004:FEC:997163.997196,Cooper:2005:AAC:1065910.1065921,Ashouri:2017:MMC:3132652.3124452} have used evolutionary algorithms to solve the phase ordering problem (i.e.\ in which order a set of compiler transformations should be applied).

\textit{Machine Learning and Parallelization.}
Most relevant to the work presented in this paper is the approach of Fried \emph{et al.}~\cite{fried_ea:2013:icmla}. Similar to our approach, Fried \emph{et al.} train a supervised learning algorithm on code hand-annotated with OpenMP parallelization directives in order to approximate the parallelization that might be produced by a human expert. However, we do not rely solely on OpenMP annotations, but we complement our training set with substantially richer data obtained from an aggressively configured parallelizing compiler. While Fried \emph{et al.}~focus on the comparative performance of different ML algorithms, we contribute a practical parallelization assistant capable of ranking loop candidates in their order of merit. Through this we directly enhance programmer productivity in an ML-assisted environment. Similarly to our approach, Hayashi \emph{et al.}~\cite{Hayashi:2015:MPH:2807426.2807429} extracts various program features during compilation for use in a supervised learning prediction model. However, its aim is the optimal runtime selection of CPU vs. GPU execution and it is limited to programs written in Java using the parallel stream APIs that was introduced in version 8.

\section{Summary \& Conclusions}

In this paper we contribute a methodology and tool for parallelization assistance. We acknowledge that parallelization is a complex process where the human expert still has a major role to play. We aim to assist the human experts by guiding them directly towards the most interesting loops, thus delivering savings for this costly human resource. We have developed a novel machine learning based approach to predicting whether or not a loop is parallelizable. We combine this prediction with traditional profiling information and develop a ranking function that prioritizes low-risk, high-gain loop candidates, which are presented to the user.

We have evaluated our parallelization assistant against the sequential C implementations of the SNU NPB suite. We show that our assistant recognizes parallelizable loops more aggressively than conservative parallelizing compilers. We also show that our parallelization assistant can increase programmer productivity. Our experiments confirm, that equipped with our assistant, a programmer is required to examine and parallelize substantially fewer loops to achieve performance levels comparable to those of the reference OpenMP implementations of the benchmarks.

Our work has demonstrated that there is scope for machine learning based tool support in parallelization despite its inherent lack of safety. By assisting human programmers rather than replacing them, machine learning techniques have the potential to deliver productivity gains beyond what is possible by relying on traditional parallelization approaches alone.

\section*{Acknowledgements}

This work was supported by grant EP/L01503X/1 for the University of Edinburgh School of Informatics Centre for Doctoral Training in Pervasive Parallelism. We would like to thank Artemiy Margaritov and Kuba Kaszyk for their valuable feedback on earlier drafts of this paper. Finally, we would like to thank the anonymous reviewers for their comments and suggestions.

%The ultimate goal of this work has always been to provide a programmer with a feedback on the parallelisability of software in general and loops in particular. The work has evolved from parallelisability correlations of single metrics (features) to a machine learning based tool for filtering runtime program profile and providing the resulting loop ranking for a programmer.\newline\null
%\quad The ultimate performance of the tool depends on many factors. First 

%\subsection{Future Work}

%Having received some motivating results, there are still certain areas of improvement. The tool consists of a number of co-designed parts. Their tuning for maximum overall performance can be done infinitely. The ultimate performance of the tool is affected by a number of factors. The main factor is the nature of the programs (benchmarks) being used for ML training and testing stages. SNU NAS benchmarks are quite diverse in terms of the loops they contain. Loops have different sizes, nesting structures, parallelism patterns.

%\quad We believe, that our idea can be worked on and improved even further. There are several possible potential steps.

%\quad Our scheme consists of a number of components and steps. First we select training and testing programs, then we engineer the set of representative ML features. After 

%\quad First, SNU NPB benchmarks have certain features and properties that reveal themselves in our work. 
%\quad In our work we use SNU NAS Parallel Benchmarks for assessment of our machine learning based technique. Majority of SNU NPB benchmarks concentrate their running time in a relatively small set of critical loops. That fact presents a problem for a full scale assessment of our technique.      



%Our tool consists of a lot of components to be tuned. The tuning work can be done infinetely in the attempt to get the best possible performance on a wider range of programs and benchmarks.    

% \subsection{Future Work}







