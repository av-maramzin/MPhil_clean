\chapter{Introduction}
\label{introduction}

\section{Overview}
\label{introduction_overview}

% The importance of software parallelization

\quad Parallelism has become pervasive in the world of computing. Parallel hardware is omnipresent across the whole spectrum of various computing systems from low-end embedded processors to high-end supercomputers. Yet, most of the existing software is written in a sequential fashion: be it an old legacy software initially designed to run on the available at that time serial hardware or modern applications being developed by application domain experts rather than performance engineers. In order to exploit all available hardware facilities software has to be parallelized.

% The challenges of software parallelization

%%  Manual parallelization challenges

\quad The task of \textbf{software parallelization} is multifaceted, extremely complex and has characteristically been a very manual process, which is time consuming and error-prone. An embarrassingly parallel problem might end up being hidden behind a thoughtless software design, a serial algorithm or being implemented with an unsuccessfully chosen lover level constructs, such as pointers, heap-allocated and pointer-linked data structures, indirect array referencing, etc. Legacy code bases are extremely rich with all these sub-optimal from the angle of software parallelization programming decisions.

To elegantly and effectively map a parallel problem onto the exact hardware a programmer must possess an expert-level knowledge in the range of various fields from software design and algorithmic patterns in software engineering down to compiler's automatic vectorization and hardware cache coherence protocols. It is not always realistic to expect such a deep expertise from an average programmer.

Done in the wrong way software parallelization can even slow the program down in comparison to its original sequential version. 

\newline\null
\quad Despite decades of research into various alternative to the manual software parallelization methods, the "silver bullet" has not yet been found and all methods face a number of challenges.

%%  Automatic parallelization challenges

For decades researches and engineers have been working on alleviating the challenging task of software parallelization. One of these work directions is automatic parallelization techniques, which are supposed to ultimately liberate programmers from the challenging task of manual parallelization. But, despite the massive body of work on the subject, automatic techniques still cannot deliver performance levels expert programmers reach and are limited to narrow domains of scientific Fortran codes and relatively simple computational idioms.\newline\null

%% Challenges of machine learning based methods

There has even been a research into utilizing exotic to the field of software parallelization machine learning based methods. Although these methods have proved to be extremely useful and high performing on some compilation technology problems like selecting the best compiler flags or finding the most optimal compiler optimization parameters (like loop unroll or function inline factors). Due to the inherent statistical errors and unavailability of large training data sets these methods have not yet found a widespread application in the area of software parallelization. 



% The solution we propose
we do not expect a breakthrough in the area in the near future.  or
We do not strive to find a "silver bullet" and solve the task of automatic parallelization, neither do we try to tune machine learning prediction accuracy to 100\% perfection. We do not expect an average programmer to be an expert either. 

Instead, we acknowledge the role a programmer plays in the parallelization process and equip the former with an \textit{assistant solution}. Our solution alleviates the task and makes it more accessible for an average programmer. 

\quad The assistant solution consists of a tool and a library aiming at different stages of software parallelization. 


\quad The assistant solution we propose is as multifaceted as the problem itself. To fully exploit all the potential of software parallelization a programmer has to work on several conceptual levels. 

\quad The assistant solution we propose is as multifaceted as the problem itself and consists of several components aimed to tackle different aspects of software parallelisability. It is a toolkit rather than a single tool. The first tool in the toolkit has been designed, developed and assessed during the first year of my PhD. Complementary tools are supposed to be developed over the course of remaining time.\newline\null

\section{Loop parallelization assistant}
\label{introduction_assistant}
\quad The tool aims at finer levels of granularity. Program loops are often the richest source of parallelism and account for the biggest portion of the running time. The tool identifies those loops, which are both worthwhile and feasible to parallelize. For each loop the tool combines its potential contribution to speedup and an estimated probability for its successful parallelization. This probability is predicted using a machine learning model, which has been trained and tested on 1415 labelled loops, achieving a prediction accuracy greater than 90\%. We present a methodology which makes a better use of an expert time by guiding them directly towards those loops, where the largest performance gains can be expected while keeping analysis and transformation effort at a minimum. We have evaluated our parallelization assistant against sequential C applications from the SNU NAS benchmark suite. We show that our novel methodology achieves parallel performance levels comparable to those from expert programmers while requiring less expert time. On average, our assistant reduces the number of lines of code that have to be inspected manually before reaching expert-level parallel speedup by 20\%.

\quad The designed tool is basically a machine learning (ML) based model of loop parallelisability integrated and utilised into an assistant scheme. Assistant takes an application to be parallelised along with its profile as an input and presents a programmer with a ranking of application loops. In contrast to a profile guided approach, assistant's ranking highlights application loops, which are not only long-running (and thus potentially profitable), but crucially parallelisable. This way a programmer concentrates his efforts on promising application loops and converges to the best achievable performance faster. We have deployed our assistant on SNU NAS Parallel Benchmarks \cite{snu-npb-benchmarks} \cite{nasa-parallel-benchmarks} and demonstrated a potential of our idea \cite{aiseps}.\newline\null

\quad Another tool from our solution package work on the lower final implementation level and aims at finer-grained parallelism, specifically program loops. The designed tool is basically a machine learning (ML) based model of loop parallelisability integrated and utilised into an assistant scheme. Assistant takes an application to be parallelised along with its profile as an input and presents a programmer with a ranking of application loops. In contrast to a profile guided approach, assistant's ranking highlights application loops, which are not only long-running (and thus potentially profitable), but crucially parallelisable. This way a programmer concentrates his efforts on promising application loops and converges to the best achievable performance faster. We have deployed our assistant on SNU NAS Parallel Benchmarks \cite{snu-npb-benchmarks} \cite{nasa-parallel-benchmarks} and demonstrated a potential of our idea \cite{aiseps}.

\subsection{Contributions}
\quad In summary, the project of our machine learning based loop parallelization assistant makes the following contributions:
%
\begin{itemize}
\renewcommand\labelitemi{$\vartriangleright$}
\renewcommand\labelitemii{$\bullet$}
\item We introduce a machine learning model, which can be used to predict the probability with which sequential C loops can be parallelized (Sections~\ref{predicting_parallel_loops} and~\ref{ml_predictive_performance});
\item we integrate profiling of execution time with our novel ML model into a parallelization assistant, which guides the user through a ranked list of loops for parallelization (Section~\ref{practical_applications}); and
\item we demonstrate that our tool and methodology increase programmer productivity by identifying parallel loop candidates better than existing state-of-the-art approaches (Section~\ref{evaluation}).
\end{itemize}


\section{Computational frameworks library}
\label{introduction_frameworks}

% The problem

%% Data-centric parallelization problem

\quad Among all lower level implementation questions, the problem of successfull data structure choice stands particularly prominent and important. Listings \ref{lst:introduction_array} and \ref{lst:introduction_list} illustrate the how easily the parallelization can be hampered.
\begin{minipage}[t]{0.4\linewidth}
\begin{lstlisting}[caption={Parallelisable loop operating on a \textbf{linear array}.},label={lst:introduction_array},language=C]
for (int i=0; i<n; it++) {
  a[i]=a[i]+1;
}
\end{lstlisting}
\end{minipage}
%
\begin{minipage}[t]{0.6\linewidth}
\begin{lstlisting}[caption={Non-parallelisable loop operating on a \textbf{linked-list}.},label={lst:introduction_list},language=C]
for (p=list; p!=NULL; p=p->next) {
  p->value+=1;
}
\end{lstlisting}
\end{minipage}


%% Literature review: unavailability of automatic data structure recognizers

addressing the problem of unsuccessful data structure choice,
 The tool is at the literature review and feasibility study stage.
 
%% SPEC CPU2006 complexity level example

% The solution
There are real world benchmarks demonstrating that it is not always really necessary to separate data structures from the algorithms.

The library implements the novel notion of \textit{computational frameworks}. The latter help a programmer with a coarse-grained parallelization tasks, such as software design, algorithm and data structure choice. The library provides a user with a well-designed, modern and convenient interface. We prototyped the library on the suite of Olden benchmarks. The parallel library version is consistently outperforming the sequential version hitting 5-6x speedups on the major benchmarks.\newline\null

\quad The programmer starts on the high level of problem domain and algorithm. The task is to find independent parts in the problem to be solved in parallel. If there are dependencies among them, then the programmer has to design a synchronization mechanism. In our solution package we propose an off-the-shelf C++ template library of computational frameworks. The latter are ready to use parallelism patterns. A programmer only needs to customize his computation through an LLVM-like interface.

\subsection{Contributions}
\begin{itemize}
\renewcommand\labelitemi{$\vartriangleright$}
\renewcommand\labelitemii{$\bullet$}
\item We introduce a machine learning model, which can be used to predict the probability with which sequential C loops can be parallelized (Sections~\ref{predicting_parallel_loops} and~\ref{ml_predictive_performance});
\item we integrate profiling of execution time with our novel ML model into a parallelization assistant, which guides the user through a ranked list of loops for parallelization (Section~\ref{practical_applications}); and
\item we demonstrate that our tool and methodology increase programmer productivity by identifying parallel loop candidates better than existing state-of-the-art approaches (Section~\ref{evaluation}).
\end{itemize}

